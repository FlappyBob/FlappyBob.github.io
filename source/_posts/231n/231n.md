---
title: 231n log 
date: 2024-12-16 23:52:07
categories: 
- learn 
---

## 摸爬滚打的学习小tip
* 刚刚上手一门新课的时候写cheatsheet挺有用的，遇到一个module想把它存到自己脑子的cache里，写下来比潦草看一遍进脑子有效多了，别看这方法笨，手动的快一点其实速度不差的。当然，到信手拈来的时候就没记录必要了，但是一开始学是有必要的，我脑子笨。
* 如果要长期写某项语言，最好还是自己开个online terminal用下（数值计算课还真是大爹），当然，拿blog写个cheatsheet。 
* chatbot是好帮手。一开始对一个项目是什么都不了解的时候，比如一开始KNN的dataload和数据的dimension我就一点都没概念。直接上手放gpt里，一行一行读，gpt不详细的就google，然后写cheatsheet，这样效率非常高。
* 先在纸上演算再写代码。
* 然后写代码的时候也多多以矩阵思维思考，写代码的时候先考虑能不能先broadcast。比如这种的`dscores = probs; dscores[range(num_examples),y] -= 1; dscores /= num_examples`，how to vectorize calculation？see this: https://cs231n.stanford.edu/vecDerivs.pdf。
* 看笔记就足够了，然后直接写作业
* 笔记最后有paper可以读
* 手算softmax loss导的时候挺struggle的（虽然结果很简洁），估计之后也有不少，可以看下知乎上的`矩阵求导术`。

### Assignment 1 完结
本np新手觉得相当保姆啊。从fully for loop codes to vectorized codes的实现。同时前三个小作业做的过程中能很直观的理解为什么要写矩阵code（快）

整个课程的流程从architecture(svm/softmax loss到gradient到network forward/backward，到最后用这些积木搭建一个简化的NN)的设计，到设计优化过程（sgd, validation set来tune超参），然后对这些步骤逐步模块化(从全部塞在notebook里，到, 塞进layers.py然后用layer的积木搭fc_net/ 到最后的solver.py / optim.py的调参炼丹术，并且彻底模块化)的过程都让我这个新手感觉非常保姆，因为这节课也是工程导向的，我认为设计得非常的好。

### Assignment 2 

#### 一些好习惯
* **循序渐进**。
  * 代码中循序渐进的例子有很多：写loss的时候set mode。
``` py
def loss(self, X, y=None):
        """Compute loss and gradient for the fully connected net.
        
        Inputs:
        - X: Array of input data of shape (N, d_1, ..., d_k)
        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].

        Returns:
        If y is None, then run a test-time forward pass of the model and return:
        - scores: Array of shape (N, C) giving classification scores, where
            scores[i, c] is the classification score for X[i] and class c.

        If y is not None, then run a training-time forward and backward pass and
        return a tuple of:
        - loss: Scalar value giving the loss
        - grads: Dictionary with the same keys as self.params, mapping parameter
            names to gradients of the loss with respect to those parameters.
        """
        X = X.astype(self.dtype)
        mode = "test" if y is None else "train"

        # ...

        if self.use_dropout:
            self.dropout_param["mode"] = mode
        if self.normalization == "batchnorm":
            for bn_param in self.bn_params:
                bn_param["mode"] = mode
        scores = None
        # ...
        
        if mode == "test":
            return scores
        
        # ...
        return loss, grads
 ```
另外还有先测layers，再写fc_net等等

* 优化中循序渐进的例子。优化的时候一般一开始都是epoch非常小确保正确性和速度之后再调回来，先确保模块正确再优化，这是可以应用在所有代码实践中的基本素质。

所以workflow尽量先遵从：
1. 在model中分mode
2. 另开一个notebook在旁边sub_feature_1
3. 然后重复，测sub_feature_1等等
4. 最后测feature_1(depends on sub_feature_{i from 1 to n})

*fc_net完结*

有了assignment1的基础之后，实现非常简单，大部分代码甚至可以在lecture notes里找到。

我认为assignment2的重心在于大规模实验中理解paper的insight。而我感觉这节课也是花了很多时间培养这些insight的，加了bn,dropout之后的实验observation，大量观察所带来的认知基础是非常有价值，很像做物理实验的赶脚。

除了实验部分很喜欢之外，我认为最有趣的部分是自己手写batch normalization图和在纸上复现那些算子的时候，让我对图和NN之间的联系产生了不少的兴趣（224w启动）

*convnet完结*

### Assignment 3

到了这部分就是researchy的部分了，大多数都和Net架构有关。

#### 一些好习惯
* 还是以作业/读代码/自我思考/看paper为主。lecture可以当小品看（让我记得最牢的是一个学生问怎么想出LSTM这种架构，justin说“这就是research” xswl）别人讲的记得不牢。
* 多在纸上画图。RNN一开始我非常不理解，但是画下来一切就在脑海中浮现出来了。以此看来记下东西的确学的更深，大脑的存储更加persistent。
