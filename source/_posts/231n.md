---
title: 231n log 
date: 2024-12-16 23:52:07
categories: 
- learn 
---

## 摸爬滚打的学习小tip
* 刚刚上手一门新课的时候写cheatsheet挺有用的，遇到一个module想把它存到自己脑子的cache里，写下来比潦草看一遍进脑子有效多了，别看这方法笨，手动的快一点其实速度不差的。当然，到信手拈来的时候就没记录必要了，但是一开始学是有必要的，我脑子笨。
* 如果要长期写某项语言，最好还是自己开个online terminal用下（数值计算课还真是大爹），当然，拿blog写个cheatsheet。 
* chatbot是好帮手。一开始对一个项目是什么都不了解的时候，比如一开始KNN的dataload和数据的dimension我就一点都没概念。直接上手放gpt里，一行一行读，gpt不详细的就google，然后写cheatsheet，这样效率非常高。
* 先在纸上演算再写代码。
* 然后写代码的时候也多多以矩阵思维思考，写代码的时候先考虑能不能先broadcast。比如这种的`dscores = probs; dscores[range(num_examples),y] -= 1; dscores /= num_examples`，how to vectorize calculation？see this: https://cs231n.stanford.edu/vecDerivs.pdf。
* 看笔记就足够了，然后直接写作业
* 笔记最后有paper可以读
* 手算softmax loss导的时候挺struggle的（虽然结果很简洁），估计之后也有不少，可以看下知乎上的`矩阵求导术`。

### Assignment 1 完结
本np新手觉得相当保姆啊。从fully for loop codes to vectorized codes的实现。同时前三个小作业做的过程中能很直观的理解为什么要写矩阵code（快）

整个课程的流程从architecture(svm/softmax loss到gradient到network forward/backward，到最后用这些积木搭建一个简化的NN)的设计，到设计优化过程（sgd, validation set来tune超参），然后对这些步骤逐步模块化(从全部塞在notebook里，到, 塞进layers.py然后用layer的积木搭fc_net/ 到最后的solver.py / optim.py的调参炼丹术，并且彻底模块化)的过程都让我这个新手感觉非常保姆，因为这节课也是工程导向的，我认为设计得非常的好。

### Assignment 2 

#### 一些好习惯
* **循序渐进**。
  * 代码中循序渐进的例子有很多：写loss的时候set mode。
``` py
def loss(self, X, y=None):
        """Compute loss and gradient for the fully connected net.
        
        Inputs:
        - X: Array of input data of shape (N, d_1, ..., d_k)
        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].

        Returns:
        If y is None, then run a test-time forward pass of the model and return:
        - scores: Array of shape (N, C) giving classification scores, where
            scores[i, c] is the classification score for X[i] and class c.

        If y is not None, then run a training-time forward and backward pass and
        return a tuple of:
        - loss: Scalar value giving the loss
        - grads: Dictionary with the same keys as self.params, mapping parameter
            names to gradients of the loss with respect to those parameters.
        """
        X = X.astype(self.dtype)
        mode = "test" if y is None else "train"

        # ...

        if self.use_dropout:
            self.dropout_param["mode"] = mode
        if self.normalization == "batchnorm":
            for bn_param in self.bn_params:
                bn_param["mode"] = mode
        scores = None
        # ...
        
        if mode == "test":
            return scores
        
        # ...
        return loss, grads
 ```
另外还有先测layers，再写fc_net等等

* 优化中循序渐进的例子。优化的时候一般一开始都是epoch非常小确保正确性和速度之后再调回来，先确保模块正确再优化，这是可以应用在所有代码实践中的基本素质。

所以workflow尽量先遵从：
1. 在model中分mode
2. 另开一个notebook在旁边sub_feature_1
3. 然后重复，测sub_feature_1等等
4. 最后测feature_1(depends on sub_feature_{i from 1 to n})

*fc_net完结*

有了assignment1的基础之后，实现非常简单，大部分代码甚至可以在lecture notes里找到。

我认为assignment2的重心在于大规模实验中理解paper的insight。而我感觉这节课也是花了很多时间培养这些insight的，加了bn,dropout之后的实验observation，大量观察所带来的认知基础是非常有价值，很像做物理实验的赶脚。

除了实验部分很喜欢之外，我认为最有趣的部分是自己手写batch normalization图和在纸上复现那些算子的时候，让我对图和NN之间的联系产生了不少的兴趣（224w启动）

*convnet完结*

### Assignment 3

到了这部分就是最精彩的部分了，大多数都和Net架构有关。

#### 一些好习惯
* 还是以作业/读代码/自我思考/看paper为主。lecture可以当小品看（让我记得最牢的是一个学生问怎么想出LSTM这种架构，justin说“这就是research” xswl）别人讲的记得不牢。
* 多在纸上画图。RNN一开始我非常不理解，但是画下来一切就在脑海中浮现出来了。以此看来记下东西的确学的更深，大脑的存储更加persistent。
    * 我感觉很多论文intuition来自对gradient flow的感知，比如LSTM和resnet的“highway”架构就解决了gradient爆炸和gradient消失的问题，而没有观察是很难“感受”到这些问题的。多多观察计算图。

#### Transformer
大部分参考的这个,transformer: https://jalammar.github.io/illustrated-transformer/
* 最好还是先学下NLP，熟悉下wordvec这些概念，一开始的word_to_idx,vocab_size,word_size搞得我头昏眼花的，
* 遇到常数差异的方程不要用for loop，多用broadcast功能来简化。
``` python 
div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-torch.log(torch.tensor(10000.0).reshape(1, 1)) / embed_dim))
pe[0, :, 0::2] = torch.sin(position * div_term)  # Even dimensions
pe[0, :, 1::2] = torch.cos(position * div_term)  # Odd dimensions
```
* 这作业开始就没手动backpropagation要求了，全是pytorch实现了，我还愣了一下。虽然课程没有要求实现，但是个人觉得收获最大的在于看transformer和它的solver的实现有很多案例学习.

decoder的forward实现
``` python
class TransformerDecoderLayer(nn.Module):
    """
    A single layer of a Transformer decoder, to be used with TransformerDecoder.
    """
    def __init__(self, input_dim, num_heads, dim_feedforward=2048, dropout=0.1):
        """
        Construct a TransformerDecoderLayer instance.

        Inputs:
         - input_dim: Number of expected features in the input.
         - num_heads: Number of attention heads
         - dim_feedforward: Dimension of the feedforward network model.
         - dropout: The dropout value.
        """
        super().__init__()
        self.self_attn = MultiHeadAttention(input_dim, num_heads, dropout)
        self.multihead_attn = MultiHeadAttention(input_dim, num_heads, dropout)
        self.linear1 = nn.Linear(input_dim, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, input_dim)

        self.norm1 = nn.LayerNorm(input_dim)
        self.norm2 = nn.LayerNorm(input_dim)
        self.norm3 = nn.LayerNorm(input_dim)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.activation = nn.ReLU()


    def forward(self, tgt, memory, tgt_mask=None):
        """
        Pass the inputs (and mask) through the decoder layer.

        Inputs:
        - tgt: the sequence to the decoder layer, of shape (N, T, W)
        - memory: the sequence from the last layer of the encoder, of shape (N, S, D)
        - tgt_mask: the parts of the target sequence to mask, of shape (T, T)

        Returns:
        - out: the Transformer features, of shape (N, T, W)
        """
        # Perform self-attention on the target sequence (along with dropout and
        # layer norm).
        tgt2 = self.self_attn(query=tgt, key=tgt, value=tgt, attn_mask=tgt_mask)
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)

        # Attend to both the target sequence and the sequence from the last
        # encoder layer.
        tgt2 = self.multihead_attn(query=tgt, key=memory, value=memory)
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)

        # Pass
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt
 ```

以及他们怎么叠起来的。
``` python
class TransformerDecoder(nn.Module):
    def __init__(self, decoder_layer, num_layers):
        super().__init__()
        self.layers = clones(decoder_layer, num_layers)
        self.num_layers = num_layers

    def forward(self, tgt, memory, tgt_mask=None):
        output = tgt

        for mod in self.layers:
            output = mod(output, memory, tgt_mask=tgt_mask)

        return output
 ```

solver的使用：loss_history怎么管理，optim怎么用。。
``` python
def _step(self):
        """
        Make a single gradient update. This is called by train() and should not
        be called manually.
        """
        # Make a minibatch of training data
        minibatch = sample_coco_minibatch(
            self.data, batch_size=self.batch_size, split="train"
        )
        captions, features, urls = minibatch

        captions_in = captions[:, :-1]
        captions_out = captions[:, 1:]

        mask = captions_out != self.model._null

        t_features = torch.Tensor(features)
        t_captions_in = torch.LongTensor(captions_in)
        t_captions_out = torch.LongTensor(captions_out)
        t_mask = torch.LongTensor(mask)
        logits = self.model(t_features, t_captions_in)

        loss = self.transformer_temporal_softmax_loss(logits, t_captions_out, t_mask)
        self.loss_history.append(loss.detach().numpy())
        self.optim.zero_grad()
        loss.backward()
        self.optim.step()
 ```


<!-- ``` python
 ``` -->

<!-- ### 完结
nice intro to dl。 -->